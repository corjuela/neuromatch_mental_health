{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "* Are the outputs the actual labels or just the loss (and if so, how do we specify the loss type)\n",
    "* Turn data into tensor prior to making datasets\n",
    "* Do we need to add beginning / end tokens to each text sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertForTokenClassification\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display \n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108141a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 31\n",
    "\n",
    "## Set the random seeds for Python and Torch\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id): #function to initalize the seeds for the workers of DataLoader\n",
    "    worker_seed = torch.initial_seed() %2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from CSV, remove index column and separate into training and test sets\n",
    "df = pd.read_csv('data/cleandata.csv')\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row,my_token):\n",
    "    try:\n",
    "       return torch.tensor(my_token.encode_plus(row)['input_ids'])\n",
    "    except:\n",
    "        with open('bad_file.txt','a') as fid:\n",
    "            fid.write(str(row.name) + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#Instantiate BERT Tokenizer and transform strings to tensors\n",
    "\n",
    "my_token = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "X_train_token = X_train.apply(lambda x : tokenize(x,my_token))\n",
    "X_test_token = X_test.apply(lambda x : tokenize(x,my_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.tensor(y_test.values) \n",
    "y_train = torch.tensor(y_train.values)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_data \u001b[39m=\u001b[39m TensorDataset(X_test,y_test)\n\u001b[1;32m      2\u001b[0m train_data \u001b[39m=\u001b[39m TensorDataset(X_train, y_train)\n\u001b[1;32m      4\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_data,batch_size \u001b[39m=\u001b[39m batch_size , shuffle \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, num_workers \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, worker_init_fn \u001b[39m=\u001b[39m seed_worker, generator \u001b[39m=\u001b[39m g_seed)\n",
      "File \u001b[0;32m~/.virtualenvs/neuro_match/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39;49m(tensors[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39m==\u001b[39;49m tensor\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "File \u001b[0;32m~/.virtualenvs/neuro_match/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "\n",
    "test_loader = DataLoader(test_data,batch_size = batch_size , shuffle = False, num_workers = 0, worker_init_fn = seed_worker, generator = g_seed)\n",
    "train_loader = DataLoader(train_data,batch_size = batch_size , drop_last = True, shuffle = True, worker_init_fn = seed_worker, generator = g_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading a pre-trained BERT model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optim = SGD(model.parameters(), lr=lr)\n",
    "my_loss = CrossEntropyLoss() #we might not need this, since the link in the word docuemt says that running the model returns a loss\n",
    "\n",
    "model.train() #set the model to train mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optim.zero_grad()\n",
    "\n",
    "y_pred,something_else = model(X_train)\n",
    "\n",
    "my_loss(y_pred,y_train)\n",
    "\n",
    "my_loss.backward()\n",
    "my_optim.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_match",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
